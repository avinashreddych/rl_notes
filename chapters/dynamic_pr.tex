\documentclass[../main.tex]{subfiles}
\begin{document}
we compute the state value function $V^{\pi}(s)$ for the policy $\pi$.  This is called the \emph{policy evaluation} problem.  We can write the Bellman equation for the state value function as

\begin{flalign}
V^{\pi}(s) &= \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_t = s \right] \nonumber \\
 &= \mathbb{E}_\pi \left[ G_t \mid s_t = s \right]  \nonumber\\
 &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid s_t = s \right] \nonumber \\
 &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^{\pi}(s_{t+1}) \mid s_t = s \right] \nonumber \\
    &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V^{\pi}(s') \right] \nonumber \\
\end{flalign}

If enviroment dynamics are known, then $V^\pi(s)$ is a simultaneous linear equations in $|S|$ unknowns. Iterative solutions are most suitable. Assume $v_0$ as the initial approximation for the state value function. Then update rule is given by the bellman equation

\begin{flalign} 
v_{k+1}(s) &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_k(s') \right] \nonumber \\
\end{flalign}

This iterative approach converges to $V^\pi(s)$. The policy evaluation problem is solved when $v_k(s) = V^\pi(s)$ for all $s \in S$.
\newline
\vspace{3pt}
\textbf{Iterative Policy Evaluation Algorithm}

\begin{algorithm}
\caption{Iterative Policy Evaluation}
\label{alg:policy_evaluation}
\begin{algorithmic}[1]
\State Input policy $\pi$, initial approximation $v_0(s) \forall s \in S$,  
\State Set discount factor $\gamma$, $k$ = 0, $V(terminal)$ = 0
\State $v_k(s) \leftarrow v_0(s)$ for all $s \in S$
\While{$|v_k - v_{k-1} | < \theta$}
\State $v_{k+1}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_k(s') \right]$ for all $s \in S$
\State $k \leftarrow k + 1$
\EndWhile
\State \textbf{return} $v_k(s)$ for all $s \in S$
\end{algorithmic}
\end{algorithm}
\vspace{3pt}
\textbf{State-Action Value Function}
\newline
The state-action value function $Q^\pi(s, a)$ is defined as the expected return starting from state $s$, taking action $a$, and then following policy $\pi$ thereafter.  We can write the Bellman equation for the state-action value function as

\begin{flalign}
Q^\pi(s, a) &= \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_t = s, a_t = a \right] \nonumber \\
 &= \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]  \nonumber\\
 &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid s_t = s, a_t = a \right] \nonumber \\
 &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma Q^\pi(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right] \nonumber \\
    &= \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q^\pi(s', a') \right] \nonumber \\
&= \mathbb{E}_\pi \left[ R_{t+1} + \gamma Q^\pi(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right] \nonumber \\
&= \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma  V^\pi(s') \right] \nonumber \\
\end{flalign}


\emph{policy improvement theorem} helps us in updating the policy once we found out the value function of a policy by policy evaluation. The theorem states that if we have a policy $\pi$ and a value function $V^\pi(s)$, then there exists a policy $\pi'$ such that $V^{\pi'}(s) \geq V^\pi(s)$ for all $s \in S$.  This means that we can always improve the value of a policy by following a greedy policy with respect to the value function.  The greedy policy with respect to the value function is defined as 

\begin{flalign}
\pi'(s) = \arg\max_{a \in \mathcal{A}} Q^\pi(s, a)
\end{flalign}
which leads to $V^{\pi'}(s) \geq V^\pi(s)$ for all $s \in S$.

\textbf{Policy Improvement Algorithm}
\newline
Assuming a deterministic policy $\pi(s) = a$

\begin{algorithm}
\caption{Policy Improvement}
\label{alg:policy_improvement}
\begin{algorithmic}[1]
\State Input policy $\pi$, value function $v(s)$ for all $s \in S$,
\State Set discount factor $\gamma$
\State Evaluate the policy $\pi$ to get $v(s)$ for all $s \in S$ using Algorithm \ref{alg:policy_evaluation} Policy Evaluation.
\State Improve the policy by following a greedy policy with respect to the value function.$\pi \leftarrow \arg\max_{a \in \mathcal{A}} Q^\pi(s, a)$ for all $s \in S $
\end{algorithmic}
\end{algorithm}

However, policy improvement algorithm involves policy evaluation. So, we need to repeat the policy evaluation and policy improvement until the policy converges. So, Value iteration algorithm updates the value function by acting greedily with respect to the value function and then improves the policy by following a greedy policy with respect to the value function. The algorithm is given below.

\textbf{Value Iteration Algorithm}
\newline
Assuming a deterministic policy $\pi(s) = a$
\begin{algorithm}
\caption{Value Iteration}
\label{alg:value_iteration}
\begin{algorithmic}[1]
\State Input policy $\pi$, initial approximation $v_0(s) \forall s \in S$,
\State Set discount factor $\gamma$, $k$ = 0, $V(terminal)$ = 0
\State $v_k(s) \leftarrow v_0(s)$ for all $s \in S$
\While{$|v_k - v_{k-1} | < \theta$}
\State $v_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_k(s') \right]$ for all $s \in S$
\State $k \leftarrow k + 1$
\EndWhile
\State \textbf{return} $v_k(s)$ for all $s \in S$
\end{algorithmic}
\end{algorithm}





\end{document}