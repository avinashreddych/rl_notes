\documentclass[../main.tex]{subfiles}

\begin{document}


\textbf{Definition}
\begin{itemize}
    \item State $s \in S$
    \item Action $a \in A$
    \item reward $r \in \mathbb{R} $
    \item Transition model $Pr(s_{t+1} | s_t, a_t)$
    \item Reward model $Pr(r |  s_{t+1}, s_t, a_t)$
    \item Discount Factor $\gamma \in [0,1]$
    \item Horizon $T$
\end{itemize}
Goal is to find a policy $\pi(a|s)$ that maximises the expectation of discounted return.
\newline
\textbf{How RL differs from MDP solutions}
\begin{itemize}
    \item No Transition Model
    \item No Reward Model
\end{itemize}
$$ J(\pi) = \mathbb{E}_\pi \left[ \sum\limits_{t=0}^T \gamma^t r_t \right]$$

However, we still solve the MDP problem using RL by interacting with the environment by learning the transition and reward models or directly learning the policy.

\textbf{Types of RL algorithms}
\begin{itemize}
    \item Model Based- if we try to learn the transition and reward models
    \item Model Free - here, we don't learn any model dynamics. No transition and reward models. Below are the types of model free RL algorithms
    \item Value Based- if we try to learn the value function $V(s)$ of the state or value function of state-action pair $Q(s,a)$.
    \item Policy Based- if we try to learn the policy $\pi(a|s) -  \pi(s,a)$ directly.
    \item Policy Gradient- if we try to learn the policy $\pi(a|s) - \pi(s,a)$ directly using gradient ascent.
    \item Actor Critic - contains both policy $\pi(a|s) - \pi(s,a)$ and value function $V(s) -  Q(s,a)$.
\end{itemize}


\end{document}